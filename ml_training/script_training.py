# # -*- coding: utf-8 -*-
# """Indonesian Ethnicity Recognition.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1AFSLSzfRyHZMGvBXzieNS8pfYDQ0-O4o

# # Pengenalan Etnis Indonesia Berdasarkan Citra Wajah Menggunakan *Gray Level Co-occurence Matrix* (GLCM) dan *Color Histogram*

# Dinda Mareta Putriany <br> 1301164558

# ## *Import Library*

# Import *library* yang digunakan
# """

# import os
# import cv2
# import xlsxwriter
# import pickle
# import numpy as np
# import pandas as pd
# import seaborn as sn
# import plotly.graph_objects as go
# import matplotlib.pyplot as plt
# from scipy.spatial import distance
# from sklearn.ensemble import RandomForestClassifier
# from skimage.feature import graycomatrix, graycoprops
# from sklearn.metrics import accuracy_score
# from sklearn.model_selection import StratifiedKFold
# from sklearn.model_selection import cross_val_score
# from sklearn.utils import shuffle
# from shutil import copyfile
# from skimage.measure import shannon_entropy
# from kneed import KneeLocator
# from sklearn.svm import SVC
# from sklearn.metrics import ConfusionMatrixDisplay
# from sklearn.model_selection import cross_validate
# from sklearn.metrics import classification_report
# from sklearn.model_selection import GridSearchCV
# from sklearn.model_selection import RandomizedSearchCV

# from google.colab import files
# uploaded = files.upload()  # pilih file zip dataset

# pip install --upgrade scikit-image numpy

# import zipfile, os

# zip_path = "/content/dataset_periorbital.zip"  # ganti dengan nama file yang diupload
# extract_path = "/content/dataset"

# with zipfile.ZipFile(zip_path, 'r') as zip_ref:
#     zip_ref.extractall(extract_path)

# os.listdir(extract_path)  # cek folder hasil ekstrak

# """## Mendefinisikan Function

# ###### Load Data
# """

# # mapping label ke nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# def load_data(data_dir):
#     data = []
#     label = []
#     idx = []
#     name = []
#     fld = []

#     classes = os.listdir(data_dir)  # ['Banjar', 'Bugis', ...]

#     for i, cls in enumerate(classes):
#         class_path = os.path.join(data_dir, cls)
#         images = os.listdir(class_path)

#         for img_name in images:
#             img_path = os.path.join(class_path, img_name)
#             img = cv2.imread(img_path)
#             if img is None:
#                 continue

#             # resize ke ukuran standar
#             img = cv2.resize(img, (400, 200))

#             data.append(img)
#             label.append(i)
#             idx.append(img_path)
#             name.append(img_name)
#             fld.append(cls)

#     return np.array(data), np.array(label), np.array(idx), np.array(name), np.array(fld)

# """###### Preprocessing"""

# # Mengubah citra RGB menjadi *grayscale* (GLCM)
# def preprocessing_glcm(data):
#     grays = []
#     for i in range(len(data)):
#         img = cv2.cvtColor(data[i], cv2.COLOR_BGR2GRAY)
#         grays.append(img)
#     return grays

# # Mengubah citra RGB menjadi HSV (Color Histogram)
# def preprocessing_color(array):
#     preprocessed=[]
#     for i in range(len(array)):
#         img = array[i].copy()
#         img=cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
#         preprocessed.append(img)
#     return preprocessed

# """###### Ekstraksi Ciri"""

# # Function feature extraction with GLCM
# def glcm_extraction(data):
#     feature = []
#     for i in range (0, len(data)):
#         distances = [1]     # Jarak dari satu tetangga ke tetangga lain
#         angles = [0, np.pi/4, np.pi/2, 3/4*(np.pi)]   # Orientasi 0, 45, 90, dan 135
#         # Proses glcm berupa correlation, symmetric, normalization
#         # Correlation, dihitung jumlah kejadian satu level nilai intensitas piksel dengan intensitas piksel lain dalam jarak dan orientasi tertentu
#         # Symmetric, membuat matriks menjadi simetris pada bagian diagonalnya
#         # Normalization, membagi setiap set pasangan piksel dengan total jumlah pasangan piksel
#         glcm = greycomatrix(data[i],
#                             distances=distances,
#                             angles=angles,
#                             symmetric=True,
#                             normed=True, levels = 256)
#         # Haralick Feature
#         properties = ['contrast','homogeneity','correlation','ASM']
#         feats = np.hstack([greycoprops(glcm, prop).ravel() for prop in properties])

#         # Entropy feature
#         entropy = [shannon_entropy(glcm[:,:,:,idx]) for idx in range(glcm.shape[3])]
#         feat = np.concatenate((entropy,feats),axis=0)
#         feature.append(feat)
#     return feature

# # Function feature extraction with Color Histogram
# def color_extraction(img):
#     features = []

#     for i in range(np.shape(img)[0]):
#         hist1 = cv2.calcHist([img[i]], [1], None, [16], [0, 256])
#         hist2 = cv2.calcHist([img[i]], [2], None, [16], [0, 256])
#         fitur = np.concatenate((hist1,hist2))
#         arr = np.array(fitur).flatten()
#         features.append(arr)
#     return features

# """###### Cross Validation"""

# # Fungsi Cross Validation menggunakan Random Forest
# def crossVal(K,X,y):
#     X,y = shuffle(X, y, random_state = 220)
#     clf = RandomForestClassifier(n_estimators=200, random_state=0)
#     cv = StratifiedKFold(n_splits=K)
#     scores = cross_val_score(clf, X, y,cv=cv)
#     return scores

# # Fungsi Cross Validation menggunakan Random Forest
# def CVSP(K,X,y):
#     X,y = shuffle(X, y, random_state = 220)
#     clf = RandomForestClassifier(n_estimators=100, random_state=0)
#     cv = StratifiedKFold(n_splits=K)
#     scores = cross_val_score(clf, X, y,cv=cv)
#     return scores

# # Fungsi Cross Validation menggunakan Random Forest
# def SVM(K,X,y):
#     scores = []
#     X,y = shuffle(X, y, random_state = 220)
#     clf = SVC(C=0.1, gamma= 1, kernel='poly')
#     cv = StratifiedKFold(n_splits=K)
#     scores = cross_val_score(clf, X, y,cv=cv)
#     return scores

# """## Berdasarkan Periorbital Wajah

# ### Load Data
# """

# data, label, idx, img_name, fld = load_data('/content/dataset/dataset_periorbital')

# print("Total Data Periorbital:", len(data))

# # 0 (Bugis) 1 (Sunda) 2 (Malay) 3 (Java) 4 (Banjar)
# periorbital = np.array(np.unique(label, return_counts=True)).T
# print("Jumlah Data Setiap Kelas:")
# periorbital

# """###### Sampel Data Periorbital"""

# # mapping label ke nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# fig, ax = plt.subplots(1, 5, figsize=(15, 5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)

# # ambil 1 index acak per kelas
# import numpy as np
# for i in range(5):
#     idx_class = np.where(label == i)[0]        # semua index dari kelas i
#     fidx = np.random.choice(idx_class)         # pilih 1 acak
#     ax[i].imshow(cv2.cvtColor(data[fidx], cv2.COLOR_BGR2RGB))
#     ax[i].set_title(label_map[int(label[fidx])])
#     ax[i].axis('off')

# plt.show()

# """###### Histogram Data Periorbital"""

# fig, ax1 = plt.subplots(1, 5, figsize=(15,5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)
# for i in range(0,5):
#     n = [11,595,1031,1482,2043]
#     hidx = n[i]
#     ax1[i].hist(data[hidx].ravel(), 256, [0,256])
#     ax1[i].set_title(label[i * 490 + (i+4) + 100])
#     ax1[i].axis('off')
# plt.show()

# """#### Preprocessing

# ##### Preprocessing GLCM
# """

# glcm_prep = preprocessing_glcm(data)

# """Sampel Hasil Preprocessing: RGB to *Grayscale*"""

# # mapping label ke nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# fig, ax = plt.subplots(1, 5, figsize=(15,5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)

# import numpy as np
# for i in range(5):
#     # ambil index semua gambar dari kelas i
#     idx_class = np.where(label == i)[0]
#     gidx = np.random.choice(idx_class)   # pilih 1 acak

#     ax[i].imshow(glcm_prep[gidx], cmap=plt.cm.gray)
#     ax[i].set_title(label_map[int(label[gidx])])
#     ax[i].axis('off')

# plt.show()

# """##### Preprocessing Color Histogram"""

# color_prep = preprocessing_color(data)

# """Sampel Hasil Preprocessing: RGB to HSV"""

# # mapping label ke nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# fig, ax = plt.subplots(1, 5, figsize=(15,5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)

# import numpy as np
# for i in range(5):
#     # ambil semua index dari kelas i
#     idx_class = np.where(label == i)[0]
#     cidx = np.random.choice(idx_class)   # pilih acak

#     ax[i].imshow(color_prep[cidx], cmap=plt.cm.hsv)
#     ax[i].set_title(label_map[int(label[cidx])])
#     ax[i].axis('off')

# plt.show()

# """### Feature Extraction

# ##### Ekstraksi ciri GLCM
# """

# # install scikit-image kalau belum
# !pip install scikit-image --quiet

# import numpy as np
# from skimage.feature import graycomatrix, graycoprops
# from skimage.measure import shannon_entropy

# def glcm_extraction(data):
#     distances = [1]
#     angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]
#     features = []

#     for img in data:
#         # GLCM matrix
#         glcm = graycomatrix(img,
#                             distances=distances,
#                             angles=angles,
#                             levels=256,
#                             symmetric=True,
#                             normed=True)

#         # Haralick properties
#         contrast = graycoprops(glcm, 'contrast')
#         homogeneity = graycoprops(glcm, 'homogeneity')
#         correlation = graycoprops(glcm, 'correlation')
#         asm = graycoprops(glcm, 'ASM')

#         # Entropy untuk tiap sudut
#         entropy_vals = [shannon_entropy(glcm[:, :, 0, j]) for j in range(len(angles))]

#         # gabung jadi satu vektor fitur
#         feat = np.hstack([
#             contrast.flatten(),
#             homogeneity.flatten(),
#             correlation.flatten(),
#             asm.flatten(),
#             entropy_vals
#         ])

#         features.append(feat)

#     return np.array(features)

# # 🚀 Sekarang ini bisa langsung jalan:
# glcm_feat = glcm_extraction(glcm_prep)
# print("GLCM feature shape:", glcm_feat.shape)

# print("Panjang Fitur GLCM:", len(glcm_feat[0]))

# """##### Ekstraksi ciri Color Histogram"""

# color_feat = color_extraction(color_prep)

# print("Panjang Fitur Color Histogram:", len(color_feat[0]))

# """### Penggabungan Fitur Tekstur dan Warna"""

# feature = np.concatenate((glcm_feat,color_feat),axis=1)

# print("Panjang Fitur Gabungan:", len(feature[0]))

# """### Pengujian"""

# # Generate k
# var = [int(x) for x in np.linspace(start = 2, stop = 20, num = 19)]
# k = np.array(var)

# """##### Pengujian dengan Random Forest"""

# # Pengujian dengan menggunakan k=2 sampai k=10
# acc_rf = []
# for i in range(len(k)):
#     a1 = crossVal(k[i],feature,label)
#     s1 = np.mean(a1)*100
#     acc_rf.append(s1)
#     print("Rata-rata akurasi", str(k[i]),": ",s1)
#     i += 1

# """Mencari k belokan (optimal) menggunakan knee locator"""

# xr = list(range(2, 2+len(acc_rf)))  # buat x yang pas dengan panjang acc_rf
# yr = acc_rf

# plt.plot(xr, yr, marker='o', label="Nilai k")

# # KneeLocator
# from kneed import KneeLocator
# KLr = KneeLocator(xr, yr, S=1.0, curve='concave', direction='increasing')

# xkneer = KLr.knee
# ykneer = yr[xr.index(xkneer)]

# plt.plot(xkneer, ykneer, "ro", label="Nilai k Optimal")

# plt.xlabel('k')
# plt.ylabel('akurasi (%)')
# plt.title('Pengujian Nilai k pada Random Forest')
# plt.legend()
# plt.ylim([80, 100])
# plt.show()

# print("Nilai k Optimal:", xkneer)

# # Akurasi berdasarkan k optimal
# rf = crossVal(6,feature,label)
# print("Rata-rata akurasi : ",np.mean(rf)*100)

# """##### Pengujian dengan SVM"""

# # Pengujian dengan menggunakan k=2 sampai k=10
# acc_svm = []
# for i in range(len(k)):
#     a2 = SVM(k[i],feature,label)
#     s2 = np.mean(a2)*100
#     acc_svm.append(s2)
#     print("Rata-rata akurasi", str(k[i]),": ",s2)
#     i += 1

# """Mencari k belokan (optimal) menggunakan knee locator"""

# xs = k
# ys = acc_svm
# # plotting the line 1 points
# plt.plot(xs, ys, marker='o', label="Nilai k")

# # Plotting knee
# KLs = KneeLocator(xs, ys, S=1.0, curve='concave', direction='increasing')
# els = KLs.knee-2

# yknees = acc_svm[els]
# xposs = ys.index(yknees)
# xknees = xs[xposs]
# plt.plot(xknees, yknees, "ro",label = "Nilai k Optimal")

# # naming the x axis
# plt.xlabel('k')
# # naming the y axis
# plt.ylabel('akurasi')
# # giving a title to my graph
# plt.title('Pengujian Nilai k pada SVM')
# plt.legend()
# plt.ylim([80, 100])
# # function to show the plot
# plt.show()

# print("Nilai k Optimal: ",str(xknees))

# # Akurasi berdasarkan k optimal
# svm = SVM(6,feature,label)
# print("Rata-rata akurasi : ",np.mean(svm)*100)

# """### Hasil Pengujian"""

# # Membuat kombinasi nama folder + nama file
# fname = [f"{fld[i]}_{img_name[i]}" for i in range(len(img_name))]

# print("Contoh 10 fname pertama:")
# print(fname[:10])

# # Membuat confusion matrix setiap fold
# xt, yt, tidx, pred = [], [], [], []
# X, y, idx_data, imgn = shuffle(feature,label,idx, fname, random_state =220)
# clf = RandomForestClassifier(n_estimators=200,random_state=0)
# cv = StratifiedKFold(n_splits=6)
# X = np.array(X)
# y = np.array(y)
# scoring = {'acc': 'accuracy',
#            'prec_weighted': 'precision_weighted',
#            'rec_weighted': 'recall_weighted',
#            'f1_weighted': 'f1_weighted'}

# """###### Akurasi Tiap Fold dan Akurasi Terendah"""

# print("Akurasi Tiap Fold:")
# for i in range(len(rf)):
#     acf = rf[i]
#     print("Fold ke-",i+1,':',acf*100)
# print("Akurasi Terendah",min(rf)*100)

# """### Confusion Matrix

# ###### Confusion Matrix Setiap Fold
# """

# from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

# xtra, ytra,traid=[],[],[]
# for train_index, test_index in cv.split(X,y):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]

#     clf.fit(X_train, y_train)
#     y_pred = clf.predict(X_test)
#     xtra.append(X_train)          # menyimpan X_test ke dalam array
#     ytra.append(y_train)
#     traid.append(train_index)
#     xt.append(X_test)          # menyimpan X_test ke dalam array
#     yt.append(y_test)          # menyimpan y_test ke dalam array
#     tidx.append(test_index)    # menyimpan test_index ke dalam array
#     pred.append(y_pred)        # menyimpan prediksi ke dalam array
#     ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap=plt.cm.Blues)
#     plt.show()

# """###### Confusion Matrix dari Fold yang Memiliki Akurasi Terendah

# Iterasi kedua
# """

# l_test = yt[1]
# l_pred = pred[1]
# res_data = {'y_Actual': l_test, 'y_Predicted': l_pred}

# df = pd.DataFrame(res_data, columns=['y_Actual','y_Predicted'])
# confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)
# sn.set(font_scale=1.1) # for label size
# plt.figure(figsize=(6,4))
# sn.heatmap(confusion_matrix, annot=True, annot_kws={"size": 12})
# plt.show()

# print("Jumlah Data Test:",len(l_test),'data')

# acc_min = np.array(np.unique(l_test, return_counts=True)).T
# print("Jumlah Data Setiap Kelas:")
# acc_min

# """Iterasi Keempat"""

# l_test2 = yt[3]
# l_pred2 = pred[3]
# res_data = {'y_Actual': l_test2, 'y_Predicted': l_pred2}

# df = pd.DataFrame(res_data, columns=['y_Actual','y_Predicted'])
# confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'], margins = True)
# sn.set(font_scale=1.1) # for label size
# plt.figure(figsize=(6,4))
# sn.heatmap(confusion_matrix, annot=True, annot_kws={"size": 12})
# plt.show()

# print("Jumlah Data Test:",len(l_test2),'data')

# acc_min = np.array(np.unique(l_test2, return_counts=True)).T
# print("Jumlah Data Setiap Kelas:")
# acc_min

# """### Classification Report"""

# sc = cross_validate(clf, X, y, scoring=scoring,cv=cv)

# """###### Rata-Rata Akurasi, Precision, Recall, dan F1 Score"""

# print("Rata-rata nilai akurasi:",np.mean(sc['test_acc']*100))
# print("Rata-rata Precision Weighted:",np.mean(sc['test_prec_weighted']*100))
# print("Rata-rata Recall Weighted:",np.mean(sc['test_rec_weighted']*100))
# print("Rata-rata F1 Score Weighted:",np.mean(sc['test_f1_weighted']*100))

# """###### Classification Report Setiap Fold"""

# report = []
# for i in range(0,6):
#     cr = classification_report(yt[i], pred[i])
#     report.append(cr)

# print(report[1])

# """###### Calssification Report Fold Dengan Akurasi Rendah"""

# # Iterasi kedua
# print(classification_report(l_test, l_pred))

# # Iterasi keempat
# print(classification_report(l_test2, l_pred2))

# """### Data Yang Salah Terprediksi

# ###### Iterasi kedua
# """

# S_pred = np.asarray(np.where(l_test != l_pred)) #Mencari data yang salah terprediksi

# tst_idx = tidx[1]                # test index fold dengan akurasi terendah
# for i in range(len(S_pred)):
#     idv = S_pred[i]      # get index salah prediksi
#     act = l_test[idv]    # get label aktual dari index salah prediksi
#     prd = l_pred[idv]    # get label prediksi dari index salah prediksi
#     dt_idx = tst_idx[idv]        # get index data test from test_index
#     data_act = idx_data[dt_idx]     # get index data actual form idx_data

# print("Jumlah Data Salah Prediksi:",len(idv),"data")

# """###### Iterasi Keempat"""

# S_pred2 = np.asarray(np.where(l_test2 != l_pred2)) #Mencari data yang salah terprediksi

# tst_idx2 = tidx[3]                # test index fold dengan akurasi terendah
# for i in range(len(S_pred2)):
#     idv2 = S_pred2[i]      # get index salah prediksi
#     act2 = l_test2[idv2]    # get label aktual dari index salah prediksi
#     prd2 = l_pred2[idv2]    # get label prediksi dari index salah prediksi
#     dt_idx2 = tst_idx2[idv2]        # get index data test from test_index
#     data_act2 = idx_data[dt_idx2]     # get index data actual form idx_data

# print("Jumlah Data Salah Prediksi:",len(idv2),"data")

# """###### Index Data Sesungguhnya (Dataset) Yang Salah Terprediksi"""

# # Iterasi kedua
# print("Index data salah prediksi:",data_act)

# # Iterasi keempat
# print("Index data salah prediksi:",data_act2)

# """###### Tabel Label Data Yang Salah Terprediksi"""

# fig = go.Figure(data=[go.Table(header=dict(values=['<b>Wrong Predict Index</b>','<b>Actual Class</b>','<b>Predicted Class</b>'
#     ,'<b>Test Index (After Random)</b>','<b>Index Data</b>'],
#     line_color='darkslategray',fill_color='grey',align='center',font=dict(color='white', size=12)),
#   cells=dict(values=[idv,act,prd,dt_idx,data_act],line_color='darkslategray',
#     # 2-D list of colors for alternating rows
#     fill_color = [['white']*5],align = ['center'],font = dict(color = 'darkslategray', size = 12)))])
# fig.update_layout(width=800, height=600)
# fig.show()

# fig = go.Figure(data=[go.Table(header=dict(values=['<b>Wrong Predict Index</b>','<b>Actual Class</b>','<b>Predicted Class</b>'
#     ,'<b>Test Index (After Random)</b>','<b>Index Data</b>'],
#     line_color='darkslategray',fill_color='grey',align='center',font=dict(color='white', size=12)),
#   cells=dict(values=[idv2,act2,prd2,dt_idx2,data_act2],line_color='darkslategray',
#     # 2-D list of colors for alternating rows
#     fill_color = [['white']*5],align = ['center'],font = dict(color = 'darkslategray', size = 12)))])
# fig.update_layout(width=800, height=600)
# fig.show()

# # Create a workbook and add a worksheet.
# workbook = xlsxwriter.Workbook('featuri.xlsx')
# worksheet = workbook.add_worksheet()
# # Add a bold format to use to highlight cells.
# bold = workbook.add_format({'bold': True})
# # Some data we want to write to the worksheet.
# expenses = []
# # sh = ['B1','C1','D1','E1','F1','G1','H1','I1']
# # hn = ['Data 1','Data 2','Data 3','Data 4','Data 5','Data 6','Data 7','Data 8']
# a=0
# for i in range(len(feature)):
#     f = feature[i]
#     for j in range(len(f)):
#         # Write some data headers.
# #         worksheet.write(sh[i],hn[i], bold)
#         # Start from the first cell below the headers.
#         row = 1+a
#         col = 1
# #         exp = f[j]
#         # Iterate over the data and write it out row by row.
#         for item in (f):
#             worksheet.write(row, col,  item)
#             col += 1
#         a+=1
# workbook.close()

# """###### Citra Yang Salah Terprediksi

# ###### Iterasi kedua
# """

# import random

# # Mapping angka label → nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# # ambil 8 index acak dari seluruh dataset
# idx_samples = random.sample(range(len(data)), 8)

# fig, ax = plt.subplots(2, 4, figsize=(15,5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)

# for n, idx in enumerate(idx_samples):
#     i, j = divmod(n, 4)
#     img = cv2.cvtColor(data[idx], cv2.COLOR_BGR2RGB)
#     suku = label_map[int(label[idx])]

#     ax[i,j].imshow(img)
#     ax[i,j].set_title(suku)
#     ax[i,j].axis('off')

# plt.show()

# import os

# out_dir = r"D:\Documents\Tugas_Akhir\Hasil_Prediksi(Salah)"
# os.makedirs(out_dir, exist_ok=True)   # bikin folder kalau belum ada

# nam = []
# for i in range(len(data_act)):
#     ct = data_act[i]

#     # kalau data_act isinya indeks
#     if isinstance(ct, (int, np.integer)):
#         lbl = label[ct]
#         fn = str(lbl) + "_" + str(fname[ct])
#         cv2.imwrite(os.path.join(out_dir, fn), data[ct])
#         nam.append(fn)

#     # kalau data_act isinya path file
#     elif isinstance(ct, str):
#         # cari index berdasarkan fname yang cocok
#         if ct in fname:
#             idx = fname.index(ct)
#             lbl = label[idx]
#             fn = str(lbl) + "_" + str(fname[idx])
#             cv2.imwrite(os.path.join(out_dir, fn), data[idx])
#             nam.append(fn)

# """###### Iterasi keempat"""

# import random

# # Mapping angka label → nama suku
# label_map = {0: "Bugis", 1: "Sunda", 2: "Malay", 3: "Jawa", 4: "Banjar"}

# # ambil 8 index acak dari seluruh dataset
# idx_samples = random.sample(range(len(data)), 8)

# fig, ax = plt.subplots(2, 4, figsize=(15,5))
# fig.subplots_adjust(hspace=0.5, wspace=0.1)

# for n, idx in enumerate(idx_samples):
#     i, j = divmod(n, 4)
#     img = cv2.cvtColor(data[idx], cv2.COLOR_BGR2RGB)
#     suku = label_map[int(label[idx])]

#     ax[i,j].imshow(img)
#     ax[i,j].set_title(suku)
#     ax[i,j].axis('off')

# plt.show()

# """###### Fitur GLCM Yang Salah Terprediksi"""

# gfeat_act = []
# for item in data_act:
#     # kalau item sudah berupa index integer
#     if isinstance(item, (int, np.integer)):
#         gfeat_act.append(glcm_feat[item])

#     # kalau item berupa path string
#     elif isinstance(item, str):
#         if item in fname:               # cari indexnya di fname
#             idx = fname.index(item)
#             gfeat_act.append(glcm_feat[idx])

# gfeat_act2 = []
# for item in data_act2:
#     # kalau item sudah berupa index integer
#     if isinstance(item, (int, np.integer)):
#         gfeat_act2.append(glcm_feat[item])

#     # kalau item berupa path string
#     elif isinstance(item, str):
#         if item in fname:               # cari indexnya di fname
#             idx = fname.index(item)
#             gfeat_act2.append(glcm_feat[idx])

# # ubah ke numpy array biar konsisten
# gfeat_act2 = np.array(gfeat_act2)

# data_act

# ba = 0,485
# bu = 485,950
# ja = 950,1390
# me = 1390,1845
# su = 1845,2290

# print("Contoh isi data_act:", data_act[:5])
# print("Contoh isi fname:", fname[:5])

# import os

# gfeat_act = []

# for item in data_act:
#     # kalau item index
#     if isinstance(item, (int, np.integer)):
#         if item < len(glcm_feat):
#             gfeat_act.append(glcm_feat[item])

#     # kalau item path string
#     elif isinstance(item, str):
#         base = os.path.basename(item)          # ambil nama file saja
#         # cari di fname
#         matches = [idx for idx, fn in enumerate(fname) if base in fn]
#         if matches:
#             gfeat_act.append(glcm_feat[matches[0]])

# gfeat_act = np.array(gfeat_act)
# print("Jumlah fitur berhasil masuk ke gfeat_act:", len(gfeat_act))

# from scipy.spatial import distance

# if len(gfeat_act) == 0:
#     print("❌ Masih kosong, cek isi data_act dan fname!")
# else:
#     euc, eid = [], []
#     p2 = gfeat_act[0]   # fitur acuan
#     for i in range(1845, 2290):
#         if i < len(glcm_feat):  # biar gak out of range
#             d = distance.euclidean(glcm_feat[i], p2)
#             euc.append(d)
#             eid.append(i)

#     print("Jumlah distance dihitung:", len(euc))

# for i in range(len(euc)):
#     bn = euc[i]
#     print(i,bn)

# eid[193]

# # Rata-rata euclidean distance
# mean = np.mean(euc)
# print(mean)

# # Menghitung jumlah jarak dibawah rata2 pada kelas
# n = shuffle(euc, random_state=220)
# jum = 0
# for i in range(0,400):
#     if(n[i] < mean):
#         jum += 1

# jum

# """Simpan fitur ke excel"""

# import xlsxwriter

# # Create a workbook and add a worksheet.
# workbook = xlsxwriter.Workbook('Fitur_GLCM_Salah_Prediksi.xlsx')
# worksheet = workbook.add_worksheet()

# # Add a bold format to use to highlight cells.
# bold = workbook.add_format({'bold': True})

# expenses = gfeat_act  # fitur yang mau ditulis
# n_cols = len(expenses)  # jumlah kolom sesuai data

# # Header names dinamis
# headers = [f"Data {i+1}" for i in range(n_cols)]

# # Tulis header
# for i, header in enumerate(headers):
#     worksheet.write(0, i+1, header, bold)  # baris 0, kolom i+1

# # Tulis data
# for col, exp in enumerate(expenses):
#     row = 1
#     for item in exp:
#         worksheet.write(row, col+1, item)
#         row += 1

# workbook.close()
# print("✅ File Excel berhasil dibuat dengan", n_cols, "kolom")

# """###### Fitur Color Histogram Yang Salah Terprediksi"""

# import numpy as np

# cfeat_act = []
# for item in data_act:
#     # kalau index integer
#     if isinstance(item, (int, np.integer)):
#         if item < len(color_prep):
#             cfeat_act.append(color_prep[item])

#     # kalau path string
#     elif isinstance(item, str):
#         # cocokin dengan fname
#         matches = [idx for idx, fn in enumerate(fname) if os.path.basename(item) in fn]
#         if matches:
#             cfeat_act.append(color_prep[matches[0]])

# cfeat_act = np.array(cfeat_act)
# print("Jumlah fitur warna berhasil masuk ke cfeat_act:", len(cfeat_act))

# # Iterasi keempat
# cfeat_act2 = [color_prep[i] for i in range(len(data_act2))]

# ba = 0,485
# bu = 485,950
# ja = 950,1390
# me = 1390,1845
# su = 1845,2290

# import numpy as np
# from scipy.spatial import distance

# euc = []
# eid = []

# # pilih target index valid
# target_index = min(6, len(cfeat_act) - 1)

# # flatten target jadi 1D
# p2 = np.ravel(cfeat_act[target_index])

# for j in range(1845, 2290):
#     p1 = np.ravel(color_feat[j])

#     # samakan panjang vector dengan crop atau pad
#     min_len = min(len(p1), len(p2))
#     p1 = p1[:min_len]
#     p2_resized = p2[:min_len]

#     d = distance.euclidean(p1, p2_resized)
#     euc.append(d)
#     eid.append(j)

# # Rata-rata euclidean distance
# mean = np.mean(euc)
# print(mean)

# # Menghitung jumlah jarak dibawah rata2 pada kelas
# # n = shuffle(euc, random_state=220)
# jum = 0
# for i in range(0,400):
#     if(euc[i] < mean):
#         jum += 1

# import numpy as np
# import xlsxwriter

# # Create a workbook and add a worksheet.
# workbook = xlsxwriter.Workbook('CH_Salah_Prediksi.xlsx')
# worksheet = workbook.add_worksheet()

# # Add a bold format to use to highlight cells.
# bold = workbook.add_format({'bold': True})

# # Some data we want to write to the worksheet.
# expenses = cfeat_act2
# sh = ['B1','C1','D1','E1','F1','G1','H1','I1']
# hn = ['Data 1','Data 2','Data 3','Data 4','Data 5','Data 6','Data 7','Data 8']

# for i, exp in enumerate(expenses):
#     # Write header
#     worksheet.write(sh[i], hn[i], bold)

#     # Start from first row under header
#     row = 1
#     col = i + 1  # kolom sesuai urutan

#     for item in exp:
#         # jika item array, ambil nilai scalar
#         if isinstance(item, np.ndarray):
#             if item.size == 1:
#                 value = item.item()  # scalar
#             else:
#                 value = ", ".join(map(str, item.tolist()))  # gabungkan kalau lebih dari 1 elemen
#         else:
#             value = item

#         worksheet.write(row, col, value)
#         row += 1

# workbook.close()

# # Histogram dari channel S dan V iterasi kedua
# hist1 = cv2.calcHist([cfeat_act[0]],[1],None,[16],[0,256])
# hist2 = cv2.calcHist([cfeat_act[0]],[2],None,[16],[0,256])
# plt.plot(hist1, label="S")
# plt.plot(hist2, label="V")
# plt.xlim([0,16])
# plt.legend()
# plt.xlabel("Number of Bin")
# plt.ylabel("Pixel Intensity")
# plt.show()

# # Histogram dari channel S dan V iterasi keempat
# histr1 = cv2.calcHist([cfeat_act2[0]],[1],None,[16],[0,256])
# histr2 = cv2.calcHist([cfeat_act2[0]],[2],None,[16],[0,256])
# plt.plot(histr1, label="S")
# plt.plot(histr2, label="V")
# # plt.xlim([0,16])
# plt.legend()
# plt.xlabel("Number of Bin")
# plt.ylabel("Pixel Intensity")
# plt.show()

# """### Feature Importance Random Forest"""

# # random forest for feature importance on a classification problem
# from sklearn.datasets import make_classification
# from sklearn.ensemble import RandomForestClassifier
# from matplotlib import pyplot
# # define dataset
# X,y=shuffle(feature,label,random_state=220)

# # define the model
# model = RandomForestClassifier(n_estimators = 200, random_state=0)
# # fit the model
# model.fit(X, y)
# # get importance
# importance = model.feature_importances_
# # summarize feature importance
# for i,v in enumerate(importance):
# 	print('Feature: %0d, Score: %.5f' % (i,v))
# # plot feature importance
# pyplot.bar([x for x in range(len(importance))], importance)
# pyplot.show()

# # Import pustaka yang diperlukan
# from google.colab import files

# # Nama file yang ingin diunduh
# pkl_filename = "pickle_model.pkl"

# # Perintah untuk mengunduh file ke komputer Anda
# files.download(pkl_filename)

# # Save to file dari model dengan akurasi terakhir (tertinggi)
# pkl_filename = "pickle_model.pkl"
# with open(pkl_filename, 'wb') as file:
#     pickle.dump(clf, file)